{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirillvladimirov/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/eli5/base_utils.py:28: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  init_args = inspect.getargspec(class_.__init__)\n",
      "/Users/kirillvladimirov/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/eli5/base_utils.py:36: DeprecationWarning: The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\n",
      "  return attr.s(class_, these=these, init=False, slots=True, **attrs_kwargs)  # type: ignore\n",
      "Using TensorFlow backend.\n",
      "/Users/kirillvladimirov/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/botocore/awsrequest.py:624: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class HeadersDict(collections.MutableMapping):\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from albumentations import (\n",
    "    ToFloat, \n",
    "    CLAHE, \n",
    "    RandomRotate90, \n",
    "    Transpose, \n",
    "    ShiftScaleRotate, \n",
    "    Blur, \n",
    "    OpticalDistortion, \n",
    "    GridDistortion, \n",
    "    HueSaturationValue, \n",
    "    IAAAdditiveGaussianNoise, \n",
    "    GaussNoise, \n",
    "    MotionBlur, \n",
    "    MedianBlur, \n",
    "    IAAPiecewiseAffine, \n",
    "    IAASharpen, \n",
    "    IAAEmboss, \n",
    "    RandomContrast, \n",
    "    RandomBrightness, \n",
    "    Flip, \n",
    "    OneOf, \n",
    "    Compose\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import numbers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline \n",
    "plt.style.use('seaborn')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "# classifiaction \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "\n",
    "# for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hp optimization imports\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import mlflow\n",
    "\n",
    "import re\n",
    "import eli5\n",
    "import gc\n",
    "import random    \n",
    "import math\n",
    "import psutil\n",
    "import pickle\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "# save/load models\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../data/raw/Gamma_Log_Facies_Type_Prediction/\"\n",
    "models_root = \"../../models/Gamma_Log_Facies_Type_Prediction/\"\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "pd.set_option('max_columns', 150)\n",
    "# rcParams['figure.figsize'] = 16,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16 or not. feather format does not support float16.\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 489 ms, total: 3.55 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_train_df = pd.read_csv(root + \"Train_File.csv\")\n",
    "full_test_df = pd.read_csv(root + \"Test_File.csv\")\n",
    "submit_df = pd.read_csv(root + \"Submission_File.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupKFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>well_id</th>\n",
       "      <th>GR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>143.510000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112.790928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>123.531856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>111.692784</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>123.613712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  well_id          GR  label\n",
       "0       0        0  143.510000      0\n",
       "1       1        0  112.790928      0\n",
       "2       2        0  123.531856      0\n",
       "3       3        0  111.692784      0\n",
       "4       4        0  123.613712      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4400000, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[143.51      ],\n",
       "       [112.79092812],\n",
       "       [123.53185623],\n",
       "       ...,\n",
       "       [142.73450409],\n",
       "       [140.03725205],\n",
       "       [152.4       ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df[\"GR\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df[\"GR\"] = scaler.fit_transform(full_train_df[\"GR\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.999835\n",
       "1          0.102729\n",
       "2          0.416402\n",
       "3          0.070659\n",
       "4          0.418792\n",
       "             ...   \n",
       "4399995    1.533939\n",
       "4399996    0.974187\n",
       "4399997    0.977188\n",
       "4399998    0.898418\n",
       "4399999    1.259455\n",
       "Name: GR, Length: 4400000, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df[\"GR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df[\"GR\"] = scaler.transform(full_test_df[\"GR\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.136578\n",
       "1          0.339437\n",
       "2          0.177252\n",
       "3          0.279943\n",
       "4          0.539166\n",
       "             ...   \n",
       "2199995    1.054338\n",
       "2199996    0.922949\n",
       "2199997    0.057674\n",
       "2199998    1.156047\n",
       "2199999    0.318224\n",
       "Name: GR, Length: 2200000, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test_df[\"GR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full_train_df.drop([\"label\"], axis=1)\n",
    "test = full_test_df\n",
    "y = full_train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    batch_loss = 0.0\n",
    "    batch_corrects = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "#         outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        batch_loss += loss.item()\n",
    "        batch_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    return batch_loss/len(train_loader), batch_corrects.float()/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    loss = 0.0\n",
    "    corrects = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = model(inputs)\n",
    "            _, indices = torch.max(outputs, 1)\n",
    "#             outputs = model(inputs.float())\n",
    "            loss += F.cross_entropy(outputs, labels, reduction='mean').item()\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            corrects += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "               \n",
    "    return loss/len(valid_loader), corrects.float()/len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, epochs, criterion, optimizer, scheduler):\n",
    "    epoch_loss_history = []\n",
    "    epoch_corrects_history = []\n",
    "    val_loss_history = []\n",
    "    val_corrects_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, epoch_corrects = train(model, train_loader)\n",
    "        val_loss, val_corrects = evaluate(model, valid_loader)\n",
    "\n",
    "        epoch_loss_history.append(epoch_loss)\n",
    "        epoch_corrects_history.append(epoch_corrects)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_corrects_history.append(val_corrects)\n",
    "\n",
    "        print('epoch:', (epoch+1))\n",
    "        print('training loss: {:.4f}, training acc {:.4f} '.format(epoch_loss, epoch_corrects.item()))\n",
    "        print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_loss, val_corrects.item()))\n",
    "        scheduler.step(val_corrects)\n",
    "        \n",
    "    return epoch_loss_history, epoch_corrects_history, val_loss_history, val_corrects_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histori():\n",
    "    plt.plot(epoch_loss_history, label='training loss')\n",
    "    plt.plot(val_loss_history, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(epoch_corrects_history, label='training accuracy')\n",
    "    plt.plot(val_corrects_history, label='validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4400000, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([x.values[:,2:3].T for group, x in full_train_df.groupby('well_id')], dtype='float32')\n",
    "target = np.array([x.values[:,3:].T for group, x in full_train_df.groupby('well_id')], dtype='float32')\n",
    "test = np.array([x.values[:,2:3].T for group, x in full_test_df.groupby('well_id')], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 1, 1100), (4000, 1, 1100), (2000, 1, 1100))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, target.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25124577,  0.42343712,  0.8884384 , ...,  1.0192404 ,\n",
       "         1.8522066 ,  1.458623  ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3200, 1, 1100), (3200, 1, 1100), (800, 1, 1100), (800, 1, 1100))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_valid, y_valid = train_test_split(np.array(full_train_df[\"GR\"]), np.array(full_train_df[\"label\"]),test_size=0.2, random_state=RANDOM_STATE)\n",
    "# train_df_ts.drop([\"label\"], axis=1), train_df_ts[\"label\"], \\\n",
    "#             valid_df_ts.drop([\"label\"], axis=1), valid_df_ts[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "val_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))\n",
    "\n",
    "batch_size=64\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6960, -1.4710, -1.5356, -1.7307, -1.6379],\n",
      "        [-1.9016, -1.3947, -1.4104, -1.6813, -1.7570],\n",
      "        [-1.7250, -1.4566, -1.5224, -1.6819, -1.6897],\n",
      "        [-1.8924, -1.1788, -1.7316, -1.6009, -1.8145],\n",
      "        [-1.8171, -1.2968, -1.6683, -1.5888, -1.7640],\n",
      "        [-1.8076, -1.5724, -1.2833, -1.6313, -1.8604],\n",
      "        [-2.1081, -1.3262, -1.3959, -1.5951, -1.8168],\n",
      "        [-1.9318, -1.2868, -1.3251, -1.8064, -1.9041],\n",
      "        [-1.8237, -1.6777, -1.2704, -1.6625, -1.7072],\n",
      "        [-1.7972, -1.5877, -1.4583, -1.5481, -1.6898],\n",
      "        [-1.5349, -1.5581, -1.6357, -1.6532, -1.6726],\n",
      "        [-1.6570, -1.3723, -1.7124, -1.6153, -1.7344],\n",
      "        [-1.7870, -1.3619, -1.6063, -1.7029, -1.6422],\n",
      "        [-1.5173, -1.7419, -1.6482, -1.5937, -1.5609],\n",
      "        [-1.6261, -1.5005, -1.7735, -1.4916, -1.6845],\n",
      "        [-1.7013, -1.6961, -1.4599, -1.7773, -1.4576],\n",
      "        [-1.5454, -1.7383, -1.6894, -1.5286, -1.5631],\n",
      "        [-1.7036, -1.6840, -1.3538, -1.6948, -1.6583],\n",
      "        [-1.7091, -1.5597, -1.5328, -1.6077, -1.6476],\n",
      "        [-1.9112, -1.5489, -1.3517, -1.5862, -1.7367],\n",
      "        [-1.8873, -1.4303, -1.5073, -1.6334, -1.6476],\n",
      "        [-1.8363, -1.4138, -1.4868, -1.4889, -1.9266],\n",
      "        [-1.7274, -1.4321, -1.6037, -1.7144, -1.5983],\n",
      "        [-1.8763, -1.2019, -1.5536, -1.6028, -2.0143],\n",
      "        [-1.7633, -1.1471, -1.6395, -1.7821, -1.9065],\n",
      "        [-1.8048, -1.2289, -1.5398, -1.5822, -2.0961],\n",
      "        [-1.6229, -1.3263, -1.4428, -1.7736, -2.0308],\n",
      "        [-1.6103, -1.6063, -1.2521, -1.6934, -2.0422],\n",
      "        [-1.6112, -1.5195, -1.5528, -1.7031, -1.6727],\n",
      "        [-1.5761, -1.4224, -1.5746, -1.5610, -2.0019],\n",
      "        [-1.4512, -1.6206, -1.6839, -1.8289, -1.5065],\n",
      "        [-1.4815, -1.4483, -1.4460, -1.9938, -1.7954],\n",
      "        [-1.5284, -1.3984, -1.6901, -1.6837, -1.7961],\n",
      "        [-1.5890, -1.4466, -1.5721, -1.6400, -1.8394],\n",
      "        [-1.4880, -1.4507, -1.6335, -1.6977, -1.8238],\n",
      "        [-1.4678, -1.7532, -1.6317, -1.7789, -1.4613],\n",
      "        [-1.4025, -1.5136, -1.7892, -1.7980, -1.6035],\n",
      "        [-1.3332, -1.3913, -1.8914, -1.7691, -1.7942],\n",
      "        [-1.3730, -1.5987, -1.6448, -1.7982, -1.6829],\n",
      "        [-1.4176, -1.8375, -1.7021, -1.5839, -1.5558],\n",
      "        [-1.5277, -1.7197, -1.6274, -1.5315, -1.6544],\n",
      "        [-1.4877, -1.8300, -1.5018, -1.6706, -1.5954],\n",
      "        [-1.5453, -1.5824, -1.7005, -1.6073, -1.6182],\n",
      "        [-1.6651, -1.4070, -1.5224, -1.5756, -1.9600],\n",
      "        [-1.7724, -1.2878, -1.3789, -1.7358, -2.0706],\n",
      "        [-1.4544, -1.5587, -1.6131, -1.6229, -1.8362],\n",
      "        [-1.5989, -1.4774, -1.8275, -1.4736, -1.7162],\n",
      "        [-1.3870, -1.4736, -1.8137, -1.7842, -1.6603],\n",
      "        [-1.5248, -1.4149, -1.5494, -1.7812, -1.8417],\n",
      "        [-1.5575, -1.5269, -1.4294, -1.7034, -1.8931],\n",
      "        [-1.6885, -1.7159, -1.4370, -1.6240, -1.6062],\n",
      "        [-1.5698, -1.4868, -1.6873, -1.5596, -1.7686],\n",
      "        [-1.6613, -1.5366, -1.4821, -1.6076, -1.7869],\n",
      "        [-1.4604, -1.8333, -1.6187, -1.4996, -1.6788],\n",
      "        [-1.6523, -1.6038, -1.4012, -1.5170, -1.9548],\n",
      "        [-1.6513, -1.4547, -1.6605, -1.4484, -1.8988],\n",
      "        [-1.7612, -1.5472, -1.7147, -1.4215, -1.6402],\n",
      "        [-1.7598, -1.3794, -1.6063, -1.6216, -1.7262],\n",
      "        [-1.6583, -1.4941, -1.4583, -1.7382, -1.7338],\n",
      "        [-1.5541, -1.3949, -1.7142, -1.5185, -1.9548],\n",
      "        [-1.7891, -1.1774, -1.5401, -1.7160, -2.0349],\n",
      "        [-1.7084, -1.4315, -1.4585, -1.7027, -1.8010],\n",
      "        [-1.7835, -1.3275, -1.5398, -1.4253, -2.1898],\n",
      "        [-1.6866, -1.4000, -1.6178, -1.6109, -1.7707]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([64, 5])\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([64, 1, 1100])\n",
      "tensor([1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 1, 1, 0, 3, 4, 3, 2, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0,\n",
      "        1, 2, 2, 1, 2, 0, 2, 3, 3, 1, 2, 1, 1, 1, 1, 1])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at ../aten/src/THNN/generic/ClassNLLCriterion.c:22",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at ../aten/src/THNN/generic/ClassNLLCriterion.c:22"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, target_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out, _ = self.lstm(sentence)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "model = LSTMClassifier(1100, 64, 5)\n",
    "epochs = 50\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.8,\n",
    "    min_lr=1e-6,\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    "    patience=1,\n",
    "    threshold=1e-3\n",
    ")\n",
    "\n",
    "\n",
    "epoch_loss_history = []\n",
    "epoch_corrects_history = []\n",
    "val_loss_history = []\n",
    "val_corrects_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = 0.0\n",
    "    batch_corrects = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, indices = torch.max(outputs, 1)\n",
    "        print(outputs)\n",
    "        print(outputs.size())\n",
    "        print(labels)\n",
    "        print(labels.size())\n",
    "        print(indices)\n",
    "        print(indices.size())\n",
    "        l = criterion(outputs, labels)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        batch_loss += l.item()\n",
    "        batch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss, epoch_corrects = batch_loss/len(train_loader), batch_corrects.float()/len(train_dataset)\n",
    "    \n",
    "    loss = 0.0\n",
    "    corrects = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = model(inputs)\n",
    "            _, indices = torch.max(outputs, 1)\n",
    "            loss += F.cross_entropy(outputs, labels, reduction='mean').item()\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            corrects += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    val_loss, val_corrects = loss/len(valid_loader), corrects.float()/len(valid_dataset)\n",
    "\n",
    "    epoch_loss_history.append(epoch_loss)\n",
    "    epoch_corrects_history.append(epoch_corrects)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_corrects_history.append(val_corrects)\n",
    "\n",
    "    print('epoch:', (epoch+1))\n",
    "    print('training loss: {:.4f}, training acc {:.4f} '.format(epoch_loss, epoch_corrects.item()))\n",
    "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_loss, val_corrects.item()))\n",
    "    scheduler.step(val_corrects)\n",
    "        \n",
    "show_histori()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_dir = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.in_dim, \n",
    "            hidden_size=self.hidden_dim, \n",
    "            num_layers=self.num_layers, \n",
    "            dropout=self.dropout/2, \n",
    "            bidirectional=self.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            self.hidden_dim * 2, \n",
    "            self.hidden_dim, \n",
    "            bidirectional=self.bidirectional, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4096, int(hidden_dim)),\n",
    "            nn.SELU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(int(hidden_dim), num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.permute(2, 0, 1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        gru_out, _ = self.gru(lstm_out)\n",
    "        avg_pool_l = torch.mean(lstm_out.permute(1, 0, 2), 1)\n",
    "        max_pool_l, _ = torch.max(lstm_out.permute(1, 0, 2), 1)\n",
    "        \n",
    "        avg_pool_g = torch.mean(gru_out.permute(1, 0, 2), 1)\n",
    "        max_pool_g, _ = torch.max(gru_out.permute(1, 0, 2), 1)\n",
    "\n",
    "        x = torch.cat((avg_pool_g, max_pool_g, avg_pool_l, max_pool_l), 1)\n",
    "        y = self.fc(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(df: pd.DataFrame(), epoch_col: str, value_columns: list, y_axis_name: str, title: str):\n",
    "    \n",
    "    # code mostly based on: https://altair-viz.github.io/gallery/multiline_tooltip.html\n",
    "    plot_df = df.melt(id_vars=epoch_col, value_vars=value_columns, var_name='group', value_name=y_axis_name)\n",
    "    plot_df[y_axis_name] = plot_df[y_axis_name].round(4)\n",
    "    nearest = alt.selection(type='single', nearest=True, on='mouseover', fields=[epoch_col], empty='none')\n",
    "    line = alt.Chart().mark_line(interpolate='basis').encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "        y=f'{y_axis_name}:Q',\n",
    "        color='group:N',\n",
    "    ).properties(\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    # Transparent selectors across the chart. This is what tells us\n",
    "    # the x-value of the cursor\n",
    "    selectors = alt.Chart().mark_point().encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "        opacity=alt.value(0),\n",
    "    ).add_selection(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    # Draw points on the line, and highlight based on selection\n",
    "    points = line.mark_point().encode(\n",
    "        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
    "    )\n",
    "\n",
    "    # Draw text labels near the points, and highlight based on selection\n",
    "    text = line.mark_text(align='left', dx=5, dy=-5).encode(\n",
    "        text=alt.condition(nearest, f'{y_axis_name}:Q', alt.value(' '))\n",
    "    )\n",
    "\n",
    "    # Draw a rule at the location of the selection\n",
    "    rules = alt.Chart().mark_rule(color='gray').encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "    ).transform_filter(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    # Put the five layers into a chart and bind the data\n",
    "    return alt.layer(line, selectors, points, rules, text,\n",
    "              data=plot_df, width=600, height=300).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training):\n",
    "    valid_loss_min = np.Inf\n",
    "    patience = patience\n",
    "    # current number of epochs, where validation loss didn't increase\n",
    "    p = 0\n",
    "    # whether training should be stopped\n",
    "    stop = False\n",
    "\n",
    "    epochs = 20000\n",
    "    training_logs = []\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        # print(time.ctime(), 'Epoch:', e)\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        model.train()\n",
    "        for batch_i, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output.detach().cpu().numpy().argmax(1)\n",
    "            train_acc.append(accuracy_score(a, b))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            val_loss.append(loss.item()) \n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output.detach().cpu().numpy().argmax(1)\n",
    "            val_acc.append(accuracy_score(a, b))\n",
    "\n",
    "        if e % 100 == 0 and verbose:\n",
    "            print(f'Epoch {e}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train acc: {np.mean(train_acc):.4f}, valid acc: {np.mean(val_acc):.4f}')\n",
    "\n",
    "        training_logs.append([e, np.mean(train_loss), np.mean(val_loss), np.mean(train_acc), np.mean(val_acc)])\n",
    "\n",
    "        scheduler.step(np.mean(val_loss))\n",
    "\n",
    "        valid_loss = np.mean(val_loss)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "            p = 0\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_loss > valid_loss_min:\n",
    "            p += 1\n",
    "            # print(f'{p} epochs of increasing val loss')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break        \n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "    checkpoint = torch.load('model.pt')      \n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    if plot_training:\n",
    "        training_logs = pd.DataFrame(training_logs, columns=['Epoch', 'Train loss', 'Valid loss', 'Train accuracy', 'Validation accuracy'])\n",
    "        loss_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train loss', 'Valid loss'], y_axis_name='loss', title='Loss progress')\n",
    "        acc_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train accuracy', 'Validation accuracy'], y_axis_name='accuracy', title='Accuracy progress')\n",
    "        render(loss_plot & acc_plot)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    torch.manual_seed(42)\n",
    "    model = LSTMClassifier(10, 512, 3, 0.1, True, 9, 64)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.5)\n",
    "    model.cuda()\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=8, factor=0.5, verbose=True)\n",
    "    return model, criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net_folds(X, X_test, y, folds, plot_training, batch_size, patience, verbose):\n",
    "\n",
    "    oof = np.zeros((len(X), 9))\n",
    "    prediction = np.zeros((len(X_test), 9))\n",
    "    scores = []\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time())\n",
    "        print(train_index, valid_index)\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        train_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "        val_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)\n",
    "        \n",
    "        model, criterion, optimizer, scheduler = initialize_model()\n",
    "        model = train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training)\n",
    "        \n",
    "        y_pred_valid = []\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            p = model(data)\n",
    "            pred = p.cpu().detach().numpy()\n",
    "            y_pred_valid.extend(pred)\n",
    "            \n",
    "        y_pred = []\n",
    "        for i, data in enumerate(test):\n",
    "            p = model(torch.FloatTensor(data).unsqueeze(0).cuda())\n",
    "            y_pred.append(p.cpu().detach().numpy().flatten())\n",
    "            \n",
    "        oof[valid_index] = np.array(y_pred_valid)\n",
    "        scores.append(accuracy_score(y_valid, np.array(y_pred_valid).argmax(1)))\n",
    "\n",
    "        prediction += y_pred\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    prediction = np.array(prediction).argmax(1)\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    print('--' * 50)\n",
    "    \n",
    "    return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at 1575616543.407255\n",
      "[      0       2       3 ... 4399997 4399998 4399999] [      1      11      12 ... 4399971 4399973 4399978]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([      0,       2,       3,       4,       5,       6,       7,\\n                  8,       9,      10,\\n            ...\\n            4399990, 4399991, 4399992, 4399993, 4399994, 4399995, 4399996,\\n            4399997, 4399998, 4399999],\\n           dtype='int64', length=3519998)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-4f110928ef65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-3f8c72ea443b>\u001b[0m in \u001b[0;36mtrain_net_folds\u001b[0;34m(X, X_test, y, folds, plot_training, batch_size, patience, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'started at'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cv_projects/venv/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 raise KeyError(\n\u001b[1;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1177\u001b[0;31m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m                     )\n\u001b[1;32m   1179\u001b[0m                 )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([      0,       2,       3,       4,       5,       6,       7,\\n                  8,       9,      10,\\n            ...\\n            4399990, 4399991, 4399992, 4399993, 4399994, 4399995, 4399996,\\n            4399997, 4399998, 4399999],\\n           dtype='int64', length=3519998)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "oof, prediction = train_net_folds(train, test, y, folds, True, 64, 40, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=3, hidden_size=3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(10)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(3, 3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4208, -0.7492, -0.3161]],\n",
       "\n",
       "        [[-0.3051,  0.3074,  0.7358]],\n",
       "\n",
       "        [[ 1.9541, -0.7806, -0.0920]],\n",
       "\n",
       "        [[ 2.1447, -2.1230, -1.2172]],\n",
       "\n",
       "        [[-1.8088,  0.3749, -0.5533]],\n",
       "\n",
       "        [[ 0.3254, -1.6975, -0.7601]],\n",
       "\n",
       "        [[ 0.2054,  0.9976,  0.8525]],\n",
       "\n",
       "        [[-0.5000, -0.4228, -0.3035]],\n",
       "\n",
       "        [[-0.9898, -0.4275,  0.0078]],\n",
       "\n",
       "        [[ 0.5273, -1.5449, -1.6076]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1727, 0.2146, 0.0673]]], grad_fn=<StackBackward>),\n",
       " tensor([[[0.5718, 0.5690, 0.0858]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7703, -0.0500, -0.4288]])\n",
      "tensor([[[ 0.7703, -0.0500, -0.4288]]])\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    print(i)\n",
    "    print(i.view(1, 1, -1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-26369a5d8d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3851,  0.2380,  0.0597]],\n",
       "\n",
       "        [[ 0.3642,  0.2471,  0.0179]],\n",
       "\n",
       "        [[ 0.2595,  0.1205, -0.0535]],\n",
       "\n",
       "        [[ 0.0935,  0.0618, -0.0790]],\n",
       "\n",
       "        [[ 0.2935,  0.3007,  0.1061]],\n",
       "\n",
       "        [[ 0.3879,  0.1461,  0.1010]],\n",
       "\n",
       "        [[ 0.3556,  0.2804,  0.0649]],\n",
       "\n",
       "        [[ 0.3897,  0.2230,  0.0902]],\n",
       "\n",
       "        [[ 0.3557,  0.2325,  0.0667]],\n",
       "\n",
       "        [[ 0.4465,  0.1172,  0.1175]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 3])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 3])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b245cd0>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0514, -1.5035, -0.8481],\n",
      "        [-0.9243, -1.5360, -0.9469],\n",
      "        [-0.9562, -1.5631, -0.9010],\n",
      "        [-1.0245, -1.5138, -0.8652],\n",
      "        [-1.0046, -1.5902, -0.8441]])\n",
      "5 5\n",
      "torch.Size([5]) torch.Size([5])\n",
      "torch.Size([5, 3])\n",
      "tensor([[-1.0424, -1.4777, -0.8693],\n",
      "        [-0.9196, -1.5053, -0.9693],\n",
      "        [-0.9502, -1.5339, -0.9222],\n",
      "        [-1.0169, -1.4845, -0.8875],\n",
      "        [-0.9972, -1.5536, -0.8685]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(2000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        print(len(sentence), len(tags))\n",
    "        print(sentence_in.size(), targets.size())\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "        print(tag_scores.size())\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    break\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1731, -1.1531,  0.7127, -0.3057,  1.6711]]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6544, -1.7879,  0.4142,  0.8780, -0.8525, -0.9350,  0.2770,\n",
       "          -0.5112,  0.4880, -0.0164]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "out, hidden = lstm_layer(inp, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1100, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1100, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 66,373\n",
      "Trainable params: 66,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, LSTM, Activation\n",
    "from keras.layers import Dropout, RepeatVector, TimeDistributed\n",
    "from keras import Input, Model\n",
    "\n",
    "seq_length = 1100\n",
    "input_dims = 64\n",
    "output_dims = 5 # number of classes\n",
    "n_hidden = 64\n",
    "model1_inputs = Input(shape=(seq_length,input_dims,))\n",
    "model1_outputs = Input(shape=(output_dims,))\n",
    "\n",
    "net1 = LSTM(n_hidden, return_sequences=True)(model1_inputs)\n",
    "net1 = LSTM(n_hidden, return_sequences=False)(net1)\n",
    "net1 = Dense(output_dims, activation='relu')(net1)\n",
    "model1_outputs = net1\n",
    "\n",
    "model1 = Model(inputs=model1_inputs, outputs = model1_outputs, name='model1')\n",
    "\n",
    "## Fit the model\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
