{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import eli5\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random    \n",
    "import math\n",
    "import psutil\n",
    "import pickle\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114d8e9d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "root = \"../../data/Gamma_Log_Facies_Type_Prediction/\"\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "pd.set_option('max_columns', 150)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16 or not. feather format does not support float16.\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.82 s, sys: 583 ms, total: 3.4 s\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_train_df = pd.read_csv(root + \"Train_File.csv\")\n",
    "test_df = pd.read_csv(root + \"Test_File.csv\")\n",
    "submit_df = pd.read_csv(root + \"Submission_File.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n",
      "Memory usage of dataframe is 134.28 MB\n",
      "Memory usage after optimization is: 29.37 MB\n",
      "Decreased by 78.1%\n",
      "Memory usage of dataframe is 67.14 MB\n",
      "Memory usage after optimization is: 117.77 MB\n",
      "Decreased by -75.4%\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "reduce_mem_usage(full_train_df, use_float16=True);\n",
    "reduce_mem_usage(test_df, use_float16=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kannada_MNIST_data(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        n_pixels = 28 * 28\n",
    "        \n",
    "        if \"label\" not in df.columns:\n",
    "            # test data\n",
    "            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = None\n",
    "        else:\n",
    "            # training data\n",
    "            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = torch.from_numpy(df.iloc[:,0].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        else:\n",
    "            return self.transform(self.X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(full_train_df, test_size=0.2, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10560000 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bd5110067aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKannada_MNIST_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKannada_MNIST_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKannada_MNIST_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8d6578bf5ace>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10560000 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataset = Kannada_MNIST_data(train_df)\n",
    "valid_dataset = Kannada_MNIST_data(valid_df)\n",
    "test_dataset = Kannada_MNIST_data(test_df)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    batch_loss = 0.0\n",
    "    batch_corrects = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        batch_loss += loss.item()\n",
    "        batch_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    return batch_loss/len(train_loader), batch_corrects.float()/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    loss = 0.0\n",
    "    corrects = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss += F.cross_entropy(outputs, labels, reduction='mean').item()\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            corrects += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "               \n",
    "    return loss/len(valid_loader), corrects.float()/len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediciton(model, data_loader):\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor()\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        data = Variable(data, volatile=True)\n",
    "        data = data.to(device)\n",
    "        test_pred = test_pred.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "        \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell = nn.LSTM(input_size=10, hidden_size=10, batch_first=True)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(64, 64)\n",
    "        self.classifier = nn.Linear(64, 4)\n",
    "        \n",
    "        for m in self.lstm.children():\n",
    "            inn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        for m in self.classifier.children():\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.classifier(lstm_out.view(len(sentence), -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv2Class2Net()\n",
    "model.to(device)\n",
    "epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.7,\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    "    patience=1,\n",
    "    threshold=1e-3,\n",
    "    eps=1e-06\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch_loss_history = []\n",
    "epoch_corrects_history = []\n",
    "val_loss_history = []\n",
    "val_corrects_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_corrects = train(model, train_loader)\n",
    "    val_loss, val_corrects = evaluate(model, valid_loader)\n",
    "    \n",
    "    epoch_loss_history.append(epoch_loss)\n",
    "    epoch_corrects_history.append(epoch_corrects)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_corrects_history.append(val_corrects)\n",
    "    \n",
    "    print('epoch:', (epoch+1))\n",
    "    print('training loss: {:.4f}, training acc {:.4f} '.format(epoch_loss, epoch_corrects.item()))\n",
    "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_loss, val_corrects.item()))\n",
    "    scheduler.step(val_corrects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss_history, label='training loss')\n",
    "plt.plot(val_loss_history, label='validation loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_corrects_history, label='training accuracy')\n",
    "plt.plot(val_corrects_history, label='validation accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_pred = prediciton(model, test_loader)\n",
    "test_pred = test_pred.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_df['id']\n",
    "submission = pd.DataFrame({'id': ids,'label': test_pred.numpy().T[0]})\n",
    "submission.to_csv(path_or_buf =\"submission.csv\", index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
